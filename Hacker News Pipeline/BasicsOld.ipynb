{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker News Pipeline\n",
    "In this short data engineering focused project, we will use and demonstrate a robust data pipeline that schedules tasks in a 'correct' order (implement a directed acyclic graph), and apply it to a real world data problem. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us. For a more analytical/data science focused approach on projects, check out the Data Analysis and Data Science Github repos respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. HN is a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts. Check out its website here: https://news.ycombinator.com/.\n",
    "\n",
    "To make things easier, we have already downloaded a list of JSON posts to a file called hn_stories_2014.json. The JSON file contains a single key stories, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "* created_at: A timestamp of the story's creation time.\n",
    "* created_at_i: A unix epoch timestamp.\n",
    "* url: The URL of the story link.\n",
    "* objectID: The ID of the story.\n",
    "* author: The story's author (username on HN).\n",
    "* points: The number of upvotes the story had.\n",
    "* title: The headline of the post.\n",
    "* num_comments: The number of a comments a post has.\n",
    "\n",
    "Here's an example of the full list of keys in a story (output):\n",
    "\n",
    "<br>\n",
    "\n",
    "{\n",
    "    \"story_text\": \"\",\n",
    "    \"created_at\": \"2014-05-29T08:23:46Z\",\n",
    "    \"story_title\": null,\n",
    "    \"story_id\": null,\n",
    "    \"comment_text\": null,\n",
    "    \"created_at_i\": 1401351826,\n",
    "    \"url\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
    "    \"parent_id\": null,\n",
    "    \"objectID\": \"7815285\",\n",
    "    \"author\": \"Leynos\",\n",
    "    \"points\": 1,\n",
    "    \"title\": \"Making Twitter Easier to Use\",\n",
    "    \"_tags\": [\n",
    "        \"story\",\n",
    "        \"author_Leynos\",\n",
    "        \"story_7815285\"\n",
    "    ],\n",
    "    \"num_comments\": 0,\n",
    "    \"_highlightResult\": {\n",
    "        \"story_text\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"author\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"Leynos\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"url\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"Making Twitter Easier to Use\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        }\n",
    "    },\n",
    "    \"story_url\": null\n",
    "}\n",
    "\n",
    "<br>\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our Pipeline class. **The goal will be to find the top 100 keywords of Hacker News posts in 2014.** Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014. Naturally, repeating this process for any year of your choosing will be quite easy and a great way to catch up on current hot topics in the current year ____ (insert here) easily!\n",
    "\n",
    "First, let's get some basic things out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get pipeline module\n",
    "from pipeline import Pipeline\n",
    "\n",
    "# instantiate an instance\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entirety of our code from this project will revolve around this pipeline class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON Data\n",
    "We'll start the project by loading the JSON file data into Python. Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python dict object. We can accomplish this using the json module (documentation: https://docs.python.org/3/library/json.html).\n",
    "\n",
    "An example on how to parse JSON strings (code):\n",
    "\n",
    "<br>\n",
    "\n",
    "import json\n",
    "\n",
    "\\# notice that `sample_json` is a string, and\n",
    "\\# NOT a dict.\n",
    "sample_json = '{\"hello\": \"world\"}'\n",
    "sample_dict = json.loads(sample_json)\n",
    "print(sample_dict)\n",
    "\n",
    "\\# output:\n",
    "{'hello': 'world'}\n",
    "\n",
    "<br>\n",
    "\n",
    "To load in a file, json exposes a method called json.load() which takes in a Python file object as the first argument. Using this json.load() method, we'll load the hn_stories_2014.json file as a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get more modules\n",
    "import json\n",
    "\n",
    "# pipeline.task fcn with no arg\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    # loads the file into python dict\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    # returns list of stories\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first task will have no arguments, but every task following this wil depend on the previous one, thus creating an efficient pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the Stories\n",
    "Now that we have loaded in all the stories as a list of dict objects, we can now operate on them. Let's start by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not Ask HN posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new pipeline task that depends on file_to_json\n",
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    # new fcn that filters popular stories > 50 points, > 1 comment, no \"Ask HN\"\n",
    "    def is_popular(story):\n",
    "        # embedded fcn to return this\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (\n",
    "        # returns a generator of stories filtered\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin to see here how the pipeline would start to work - as soon as file_to_json is called, filter_stories would naturally just follow in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. **By keeping consistent data formats, each of our pipeline tasks will be adaptable with future task requirements.** That last point is especially important for any project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get new modules\n",
    "from datetime import datetime\n",
    "from pipeline import build_csv\n",
    "import string\n",
    "import io\n",
    "\n",
    "# new pipeline task that depends on filter_stories\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    # new fcn that writes the filtered JSON stories to CSV\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        # build it...\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    # return the csv\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'points', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the data formats being consistent, if we ever wanted to go back and add or change something to this task or any of the other task, it would be quite easy. Making sure of this now will save headaches in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task.\n",
    "\n",
    "The steps: \n",
    "1. Import csv, and create a csv.reader() object from the file object.\n",
    "2. Find the index of the title in the header. \n",
    "3. Iterate the through the reader, and return each item from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get csv\n",
    "import csv\n",
    "\n",
    "# new pipeline task that depends on json_to_csv\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    # steps 1, 2, 3 from above\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    \n",
    "    # returns generator of titles\n",
    "    return (line[idx] for line in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then just keep building tasks on top of each other, just like this, and the pipeline would continue to grow, without making it too much more complex. We can still very easily follow the workflow from what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like Google, google, GooGle?, and google., all mean the same keyword: google. If we were to split the title into words, however, they would all be lumped into different categories.\n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter. From the string package, we are given a handy string constant that contains all the punctuation needed.\n",
    "\n",
    "Example (code):\n",
    "\n",
    "<br>\n",
    "\n",
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "\\# output:  \n",
    "\n",
    "'!\"#%&'()*+,-./:;<=>?@[\\\\]^_\\`{|}~'\n",
    "\n",
    "<br>\n",
    "\n",
    "This is very useful so that we don't have to type out each and every possible distracting piece of characters we can think of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new pipeline task that depends on extract_titles\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    # ensure lowercase and remove characters\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        # yield (return) a generator of cleaned titles\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that yield is used here instead of return. They are both very similar, but yield is needed to return a generator (iterable) and keep local calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary\n",
    "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. \n",
    "\n",
    "Furthermore, to find actual keywords, we should enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches.\n",
    "\n",
    "Included in the project folder is a module called stop_words with a tuple of the most common used stop words in the English language. This will help with the task below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get new modules\n",
    "from stop_words import stop_words\n",
    "\n",
    "# new pipeline task that depends on clean_title\n",
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    # new fcn called that returns dict of word freq of all HN titles\n",
    "    word_freq = {}\n",
    "    # account for stop words - split titles dict on ' ' char\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            # account for 'empty' words too\n",
    "            if word and word not in stop_words:\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of other things we can do inside this function - like only taking certain words that are n characters long - depending on our needs. To beat a dead horse - this is why we need a strong, robust foundation and to keep things consistent, so that changes can happen easily in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Top Words\n",
    "Finally, we're ready to sort the top words used in all the titles. The goal is to output a list of tuples with (word, frequency) as the entries sorted from most used, to least most used. We can sort in any way we wish, but we'll just use the basic sorted() built in function for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new pipeline task that depends on build_keyword_dictionary\n",
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_words(word_freq):\n",
    "    # returns a list of\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('c', 60), ('microsoft', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('use', 51), ('make', 51), ('apple', 51), ('security', 49), ('time', 49), ('yc', 49), ('github', 46), ('nsa', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('computer', 41), ('project', 41), ('heartbleed', 41), ('1', 41), ('dont', 38), ('design', 38), ('users', 38), ('git', 38), ('ios', 38), ('twitter', 37), ('developer', 37), ('vs', 37), ('life', 37), ('os', 37), ('ceo', 37), ('big', 36), ('day', 36), ('online', 35), ('android', 35), ('years', 34), ('simple', 34), ('court', 34), ('apps', 33), ('browser', 33), ('mt', 33), ('learning', 33), ('api', 33), ('guide', 33), ('says', 33), ('mozilla', 32), ('server', 32), ('gox', 32), ('firefox', 32), ('engine', 32), ('fast', 32), ('site', 32), ('problem', 32), ('introducing', 31), ('year', 31), ('amazon', 31), ('support', 30), ('stop', 30), ('built', 30), ('million', 30), ('people', 30), ('better', 30), ('text', 30), ('does', 29), ('tech', 29), ('3', 29), ('development', 29), ('billion', 28), ('money', 28), ('chrome', 28), ('2048', 28), ('did', 28), ('inside', 28), ('developers', 28), ('library', 28)]\n"
     ]
    }
   ],
   "source": [
    "# let's run the pipeline and see our work in action!\n",
    "run = pipeline.run()\n",
    "\n",
    "# print results\n",
    "print(run[top_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the top 5 words are new, google, bitcoin, open, and programming, at least in 2014. Note that we basically just found all of this out from 2 lines of code! Once we have our pipeline setup, any time we want to redo this process for another year or a similar dataset, the output will be as simple as 1 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Analysis / Next Steps\n",
    "Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014. Nonetheless, now that you have created the pipeline, there are additional tasks you can perform with the data.\n",
    "\n",
    "While this was just a natural pipeline for a data engineering focused project, there are always nitpicky little implementations you can do to make your code run more efficiently and be more flexibile. However, since these changes and analysis are not really in the scope of a robust pipeline, we will discuss the tasks only. \n",
    "\n",
    "There are endless things you can now do with a pipeline, and again, they usually just pop up as you go along towards your intended goal. Everyone's foundation will likely look the same, but no one's code will be exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Additional Tasks\n",
    "A few changes to be made to this project:\n",
    "\n",
    "* Rewrite the Pipeline class' output to save a file of the output for each task. This will allow you to \"checkpoint\" tasks so they don't have to be run twice. (This is very important and to not be underestimated - more on this in the conclusion)\n",
    "* Use the nltk package for more advanced natural language processing tasks. (There are many advanced language processing tasks being developed right this second (unless this is the year 2200) that will vastly improve results past the basic package - check them out!)\n",
    "* Convert to a CSV before filtering, so you can keep all the stories from 2014 in a raw file. (This goes along with the first bullet point)\n",
    "* Fetch the data from Hacker News directly from a JSON API. Instead of reading from the file we gave, you can perform additional data processing using newer data. (You can get a new file from a different year for HN, or implement an API directly. For more on this - look at the Movie Review project in the Data Analysis Github repo (a full JSON API analysis is done))\n",
    "\n",
    "While these are just a few suggestions, they are enough such that the project can be re-worked in many different angles. It also should be noted that ALL of these changes shouldn't really be done together just because they are there. You should pick which change applies best to your situation and go from there. No need to implement something that won't be of use.\n",
    "\n",
    "Finally, it is also important to test each change as it happens. Don't just do everything at once. That's why running individual cells in the notebook is a thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we engineered a basic robust pipeline that can easily be followed from the first function to last, ending with pulling the top words from headlines for a given year. This pipeline enabled us to not only read the code with ease, but it also allowed us to execute it with ease (2 lines)!\n",
    "\n",
    "A couple of things to be said about this pipeline and pipelines in general:\n",
    "* A pro: The good thing about a pipeline speaks for itself: You don't need to call functions multiple times for a task. If you know that you will always follow up task A with task B, perhaps a pipeline will make things more efficient. A good schema goes a long way.\n",
    "* A con: If not done correctly, pipelines can have some roadblocks. You should always test each part of your pipeline as you implement it (not done in this project, but we should have). If you get to the end of your pipeline without testing and there is an error - yes, you will know at which point the error is thrown because the output will tell you so due to the nature of the pipeline, but this does not mean a fix in that spot will result in successful execution. In the worst case, your code will be wrong somewhere in the first function, and you will have to adjust the the entire rest of the pipeline to account for that change! \n",
    "\n",
    "With the data pipeline complete, this not only supplements data analysis and data science projects, but in some cases even makes ideas that wouldn't be viable before doable. With a proper data engineering design, even the most basic data science projects can have many different angles. As always, data engineering is often overlooked and viewed as dull in relation to its \"more interesting\" siblings of analysis and predictions, but it is just as important."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
